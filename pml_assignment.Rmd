---
title: "Practical Machine Learning Assignment"
author: "Pier Lorenzo Paracchini"
date: "27 February 2016"
output: 
  html_document: 
    keep_md: yes
    toc: yes
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
Sys.setlocale("LC_ALL", "C")
options(digits=4)
knitr::opts_chunk$set(echo = TRUE)

require(caret)
require(corrplot)
require(rpart.plot)
```

## Overview

_"Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset)."_

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. The goal of this assignment is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did the exercise. The __outcome__ of the model is the __class__ variable.

_"Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. We made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg)." [1]_

## Data

The training and testing datasets for the assignments are available at the following links:

* training, https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv 
* testing,  https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

More information about the dataset can be found at the following link (http://groupware.les.inf.puc-rio.br/har) [1].

### Loading the training dataset

Looking at the content of the file it is possible to see that there missing values "" and "NA". The initial approach is to consider "", "NA" as __NAs__.

```{r loadData, cache=TRUE}
data.training <- read.csv("pml-training.csv", stringsAsFactors = FALSE, na.strings=c("", "NA", "NULL"))
```

__Note!__ When loading the data a __simplification__ has been done, "", "NA" and "NULL" have been cosidered as __NA__ (not vailable data).

#### Creating a train and test dataset
From the training dataset `data.training` creates 2 separate datasets, train and test, that wil be used to fit and evaluate the model respectively.

```{r testTrainData, cache=TRUE}
set.seed(19711004)
inTraining <- createDataPartition(data.training$classe, times = 1, p = 0.7, list = FALSE)
trainD <- data.training[inTraining,]
testD <- data.training[-inTraining,]
```

## Exploratory Analysis & Data Transformation

Using the __train__ dataset it is possible to start to explore the available predictors in order to understand the pre-processing steps that need to be performed on the datasets in order to have an "optimal" dataset to fit the model. 

Exploration is done using only the train dataset, the transformations applied to the train dataset will be applied to the test dataset as well.

Number of observations and variables
```{r trainingDatasetDim}
dim(trainD)
```

Summary for some of the variables of the datataset   
```{r trainDatasetStructure}
#First 15 variables
summary(trainD[,1:15])
```

Looking at the summary it is possible to see that 

* some variables that have lot of missing values,

* some variables seems to be not relevant for the prediction __e.g. `r names(data.training[,1:7])`, ..__

For comodity lets split the training dataset into a predictors and outcome datasets

```{r trainDataReview}
trainD.predictors <- trainD[, -160]
trainD.outcome <- as.factor(trainD[, 160])

#Same transformation applied to test
testD.predictors <- testD[, -160]
testD.outcome <- as.factor(testD[, 160])
```

### Dealing with Missing Values
Lets find out the percentage of missing values for each variable using the following simplification, missing value == NA ...

```{r percentageMissingValues, collapse = TRUE}
percentageOfMissingValues <- function(x){
    (sum(is.na(x))/length(x))* 100
}

#Percentage of Missing Values into predictors
percMissingValue.predictors <- apply(trainD.predictors, 2, percentageOfMissingValues)
##Higher Percentages
head(percMissingValue.predictors[order(percMissingValue.predictors, decreasing = TRUE)])
##Lower Percentages
tail(percMissingValue.predictors[order(percMissingValue.predictors, decreasing = TRUE)])

#Percentage of Missing Values into outcome
percentageOfMissingValues(trainD.outcome)
```

```{r plotPercentageMissingValues}
hist(percMissingValue.predictors, main = "Distribution of % Missing Values by Predictor")
```

It is possible to see that the variables ends up in two possible bins:

* variables with mainly missing values (__percentage of missing values >90%__)
* variables without missing values (__percentage of missing values 0%__)

For the assignment lets start to consider only the variables that do not have missing values - lets keep those variables

```{r keepVariablesWithoutMissingValues}
idx_variablesToKeeps <- which(percMissingValue.predictors == 0)
trainD.predictors <- trainD.predictors[, idx_variablesToKeeps]

#Same transformation applied to Test
testD.predictors <- testD.predictors[, idx_variablesToKeeps]
```

The simplified `trainD.predictors` dataset contains only `r dim(trainD.predictors)[2]` predictors.

### Removing not relevant predictors

Some of the predictors seem to be not relevant (at first sight) for the prediction. Specifically the following predictors __`r names(trainD.predictors[,1:7])`__ seem to contain data that is not going to be relevant for the prediction.  

```{r removeNotRelevantVariables}
idx_variablesToRemove <- 1:7
trainD.predictors <- trainD.predictors[, -idx_variablesToRemove]

#Same transformation applied to Test
testD.predictors <- testD.predictors[, -idx_variablesToRemove]
```

The simplified `trainD.predictors` dataset contains only `r dim(trainD.predictors)[2]` predictors.

### Transformation on Individual Predictors

The initial idea is to predict using trees for the prediction. Trees are in general more resilent to skewness and scaling/ centering issues than other models. For this reason those issues will not be considered for the time being.

Focus will be given to remove near-zero-variance predictors and higly correlated predictors.

#### Near-Zero Variance Predictors

For the selected predictors there are not prolematic predictors with Near-Zero Variance.

```{r nearZeroVariance, collapse=TRUE}
nsv <- nearZeroVar(trainD.predictors, saveMetrics = TRUE)
nsv
```

#### Between-Predictor Correlation

Lets calculate the correlation between predictors

```{r predictorsCorrelations, collapse=TRUE}
trainD.predictors.corr <- cor(trainD.predictors)
```

and let visually examine the corretion structure between the predictors

```{r coorelationPlot}
par(cex = 0.6)
corrplot(trainD.predictors.corr, order = "hclust")
```

from the plot is possible to see that there are cluster of highly correlated predictors (see dark blue and dark red points). Another possible simplification is toremove the highly correlated predictors

```{r higlyCorrelatedPredictors, collapse=TRUE}
idx_variableToRemove.highCorr <- findCorrelation(trainD.predictors.corr, cutoff = 0.75)

#Variables with high correlation 
names(trainD.predictors[idx_variableToRemove.highCorr])
trainD.predictors <- trainD.predictors[, -idx_variableToRemove.highCorr]

#Same Transformation applied to Test
testD.predictors <- testD.predictors[, -idx_variableToRemove.highCorr]
```

```{r predictorsCorrelationsUpdated, collapse=TRUE}
trainD.predictors.corr <- cor(trainD.predictors)
par(cex = 0.7)
corrplot(trainD.predictors.corr, order = "hclust")
```

The simplified `trainD.predictors` dataset contains only `r dim(trainD.predictors)[2]` predictors.

## Models

The idea is to use trees to predict the outcome. Selection of the trees is mainly connected with the easiness to interpret the model itself.

```{r dataPreparation, message=FALSE, warning= FALSE}
trainD <- data.frame(outcome = trainD.outcome, trainD.predictors)
testD <- data.frame(outcome = testD.outcome, testD.predictors)
```

### Classification Trees using `rpart`

Fitting the model based on the `train` dataset ...

```{r model1Fitting, message=FALSE, warning= FALSE}
mod1Fit <- train(outcome ~ ., method = "rpart", data = trainD)
```

```{r model1FittingShow}
print(mod1Fit$finalModel)
```

```{r model1FittingVisual,message=FALSE, warning= FALSE}
prp(mod1Fit$finalModel)
```

Using the fitted model to predict the outcome (`classe` variable) on the `test`dataset ...

```{r model1Prediction,message=FALSE, warning= FALSE}
mod1Pred <- predict(mod1Fit, newdata=testD)
```

__Evaluating__ the model ...
```{r model1Evaluation}
result1 <- confusionMatrix(mod1Pred, testD$outcome)
result1
```

Note!! The accuracy of the model is not very good, the model is performing poorly.

### Random Forests using `rf`

Fitting the model based on the `train` dataset ...

```{r model2Fitting, message=FALSE, warning= FALSE, cache=T}
library(doMC)
registerDoMC(cores = 4)
##For performance reason the number of trees has been limited to 10
mod2Fit <- train(outcome ~ ., method = "rf", data = trainD, prox = TRUE, ntree=50)
```

```{r model2FittingShow}
mod2Fit
```

Using the fitted model to predict the outcome (`classe` variable) on the `test`dataset ...

```{r model2Prediction,message=FALSE, warning= FALSE}
mod2Pred <- predict(mod2Fit, newdata=testD)
```

__Evaluating__ the model ...
```{r model2Evaluation}
result2 <- confusionMatrix(mod2Pred, testD$outcome)
result2
```

Note!! The accuracy of the model has improved dramatically compared to the prevoius model.

## Out-Of-The-Box observations

Getting the extra data and preparing it in order to be used against the models previously fitted ... 

```{r loadExtraData, cache=TRUE}
data.testing <- read.csv("pml-testing.csv", stringsAsFactors = FALSE, na.strings=c("", "NA", "NULL"))
##Removing the last variable - problem_id
extraD.predictors <- data.testing[, -160]
extraD.predictors <- extraD.predictors[, idx_variablesToKeeps]
extraD.predictors <- extraD.predictors[, -idx_variablesToRemove]
extraD.predictors <- extraD.predictors[, -idx_variableToRemove.highCorr]
```

Predicting the outcome based on the models previously fitted.

__classification tree model__

```{r predictExtra1}
predict(mod1Fit, newdata= extraD.predictors)
```

__random forests model__

```{r predictExtra2}
predict(mod2Fit, newdata= extraD.predictors)
```

## Environment Info

```{r echo=F}
sessionInfo()
```

## References

__[1]__ Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6.